---
title: "An√°lisis de Clustering de Estudiantes"
author: "Yelko Bejarano"
subtitle: Identificaci√≥n de Agrupaciones en el Comportamiento Estudiantil
output:
  html_document:
    theme: flatly
    highlight: tango
    toc: true
    toc_float: true
    code_folding: hide
    fig_width: 10
    fig_height: 6
  pdf_document:
    toc: true
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	fig.align = "center",
	message = FALSE,
	warning = FALSE,
	fig.width = 6, 
  fig.height = 4,
  dpi = 150
)

library(ggplot2)
theme_set(theme_minimal())
```

# üéØ Objetivo

Analizar si existe alg√∫n tipo de agrupaciones en cuanto al
comportamiento de los datos de estudiantes, excluyendo la variable de
identificaci√≥n Centro de cada individuo as√≠ como el identificador
individual (ID), para identificar comportamientos basados en las dem√°s
variables.

# üìä Carga y Preparaci√≥n de Datos

```{r libraries}
# Carga de librer√≠as
library(ggplot2)
library(NbClust)
library(dplyr)
library(gridExtra)
library(knitr)
library(kableExtra)
library(reshape2)
```

```{r data-loading}
# Carga de datos
datos_b <- read.csv("../data/data.csv")

# Preparaci√≥n inicial - eliminar variable de identificaci√≥n
datos_b <- datos_b[,-1]  # Eliminar ID
datos_cl <- datos_b[,-11]  # Eliminar Centro

# Vista previa de los datos
head(datos_cl) %>%
  kable(caption = "Primeras filas del dataset") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

# üîç An√°lisis Exploratorio

## Detecci√≥n de Outliers

Antes de realizar el an√°lisis cluster, se debe verificar si existen
valores outlier que puedan crear clusters con objetos muy dispersos.
Para ello se examina la distancia de Mahalanobis.

```{r outlier-detection, fig.cap="Distancia de Mahalanobis para detecci√≥n de outliers"}
# C√°lculo de distancia de Mahalanobis
ma <- mahalanobis(datos_cl, 
                  apply(datos_cl, 2, mean, na.rm = TRUE),
                  cov(datos_cl, use = "na.or.complete"))

k <- dim(datos_cl)[2]  # N√∫mero de variables
Lim <- k + 3 * sqrt(k * 2)  # L√≠mite distancia de Mahalanobis

# Gr√°fico
plot(ma, pch = 20, ylim = c(0, max(ma, Lim, na.rm = TRUE)),
     main = "Distancia de Mahalanobis",
     xlab = "Index", ylab = "Distancia")
text(ma, rownames(datos_cl), pos = 2, cex = 0.8)
abline(h = Lim, col = "red", lty = 2)
```

Se observa como ninguno de los individuos muestra una distancia por
encima del umbral, por lo que no parece haber ning√∫n outlier muy
pronunciado.

## An√°lisis de Escalas

```{r standarization, fig.cap="Comparaci√≥n antes y despu√©s de la estandarizaci√≥n"}
# Estandarizaci√≥n de datos
datos_st <- as.data.frame(scale(datos_cl, center = TRUE, scale = TRUE))

# Datos originales
datos_original <- datos_b[,c(-1,-11)]
datos_original_long <- stack(datos_original)
datos_original_long$tipo <- "Original"

# Datos estandarizados  
datos_st_long <- stack(datos_st)
datos_st_long$tipo <- "Estandarizado"

# Combinar datos
datos_combined <- rbind(
      data.frame(Variable = datos_original_long$ind, 
             Valor = datos_original_long$values, 
             Tipo = "Original"),
      data.frame(Variable = datos_st_long$ind, 
             Valor = datos_st_long$values, 
             Tipo = "Estandarizado")
  )

# Gr√°fico comparativo
ggplot(datos_combined, aes(x = Variable, y = Valor, fill = Tipo)) +
  geom_boxplot(alpha = 0.7) +
  facet_wrap(~Tipo, scales = "free_y", ncol = 1) +
  scale_fill_manual(values = c("Original" = "#A8E6CF", "Estandarizado" = "#FFB6C1")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none") +
  labs(title = "Comparaci√≥n de Escalas: Despu√©s vs Antes de Estandarizaci√≥n",
       subtitle = "La estandarizaci√≥n permite comparar variables en la misma escala",
       x = "Variables", y = "Valor")
```

Debido a la naturaleza de los datos, ya que diversas variables est√°n en
escalas que difieren de forma notable, se propone realizar una
estandarizaci√≥n de dichos datos previo al an√°lisis.

La **estandarizaci√≥n** transforma todas las variables a una escala com√∫n
(media = 0, desviaci√≥n est√°ndar = 1), permitiendo que variables con
diferentes rangos (ej: condici√≥n f√≠sica vs. horas de estudio) sean
comparables en el an√°lisis de clustering d√°ndoles la misma relevancia en
el An√°lisis de Componentes Principales.

# üå≥ An√°lisis de Clustering Jer√°rquico

Se realizar√° un an√°lisis de agrupamiento jer√°rquico utilizando la
distancia eucl√≠dea para hallar los posibles clusters.

```{r clustering-methods, fig.cap="Dendrogramas con diferentes m√©todos de clustering"}
# Matriz de distancias
D <- dist(datos_st)

# Diferentes m√©todos de clustering
hc.ward <- hclust(D, method = "ward.D2")
hc.avg <- hclust(D, method = "average")
hc.complete <- hclust(D, method = "complete")
hc.single <- hclust(D, method = "single")
```

```{r}
# Visualizaci√≥n de dendrogramas
par(mfrow = c(2,2))
plot(hc.ward, main = "ward.D2", hang = -1, cex = 0.6)
plot(hc.avg, main = "average", hang = -1, cex = 0.6)
plot(hc.complete, main = "complete", hang = -1, cex = 0.6)
plot(hc.single, main = "single", hang = -1, cex = 0.6)
par(mfrow = c(1,1))
```

Los **dendrogramas** muestran c√≥mo se agrupan los estudiantes seg√∫n cada
m√©todo:

-   **Ward.D2**: Muestra clusters equilibrados y bien definidos

<!-- -->

-   **Average**: Presenta buena separaci√≥n pero con algunos individuos
    muy alejados

-   **Complete**: Clusters m√°s compactos pero menos balanceados

-   **Single**: Tendencia a crear cadenas largas (menos √∫til para este
    an√°lisis)

## Evaluaci√≥n de M√©todos

```{r method-evaluation}
# C√°lculo de correlaciones cofen√©ticas
methods <- c("ward.D2", "average", "complete", "single")
c_coef <- sapply(methods, function(m) {
    hc <- hclust(D, method = m)
    cor(cophenetic(hc), D)
})
names(c_coef) <- methods

# Tabla de resultados ordenada de mayor a menor
resultado_metodos <- data.frame(
  M√©todo = names(c_coef), 
  Correlaci√≥n_Cofen√©tica = round(c_coef, 4)
) %>%
  arrange(desc(Correlaci√≥n_Cofen√©tica))

kable(resultado_metodos, 
      caption = "Evaluaci√≥n de m√©todos de clustering (ordenado de mejor a peor)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  row_spec(1, bold = TRUE, color = "white", background = "#28a745") %>%  # Verde para el mejor
  row_spec(2, bold = TRUE, color = "white", background = "#ffc107")     # Amarillo para el seleccionado
```

Se opta por seleccionar el m√©todo **Ward.D2** aunque muestra la segunda
mejor correlaci√≥n cofen√©tica (0.5229). Esta decisi√≥n se basa en que el
dendrograma de Ward.D2 presenta una **distribuci√≥n m√°s equilibrada e
interpretable** de los clusters comparado con el m√©todo **Average**, que
aunque tiene mejor correlaci√≥n cofen√©tica (0.6019), muestra dos
individuos muy separados del resto de grupos, creando una estructura
menos balanceada. De todos modos se har√° un an√°lisis paralelo de ambos
m√©todos para comparar posteriormente los resultados y decidir en base a
ellos.

## Determinaci√≥n del N√∫mero √ìptimo de Clusters

### Para Ward.D2

```{r optimal-clusters}
# Determinaci√≥n autom√°tica del n√∫mero de clusters
nbclust.ward <- NbClust(data = datos_st, 
                       diss = NULL, 
                       distance = "euclidean", 
                       method = "ward.D2")
```

### Para Average

```{r}
nbclust.average <- NbClust(data = datos_st, 
                          diss = NULL, 
                          distance = "euclidean", 
                          method = "average")
```

### Resultados

La funci√≥n `NbClust()` eval√∫a m√∫ltiples criterios estad√≠sticos para
determinar el n√∫mero √≥ptimo de clusters:

-   **M√©todo Ward**: La mayor√≠a de √≠ndices sugieren **2 clusters** como
    soluci√≥n √≥ptima.
-   **M√©todo Average**: Los criterios indican **3 clusters** como la
    mejor segmentaci√≥n.

Esta evaluaci√≥n autom√°tica se basa en 24 √≠ndices diferentes (Hubert,
D-index, Silhouette, etc.) que analizan la cohesi√≥n interna de los
grupos y la separaci√≥n entre ellos.

```{r dendrograms-final, fig.cap="Dendrogramas con clusters seleccionados"}
par(mfrow = c(1,2))
plot(hc.ward, main = "M√©todo Ward", hang = -1, cex = 0.6)
rect.hclust(hc.ward, k = 2, border = "blue")

plot(hc.avg, main = "M√©todo Average", hang = -1, cex = 0.6)
rect.hclust(hc.avg, k = 3, border = "blue")
par(mfrow = c(1,1))
```

Los **rect√°ngulos azules** delimitan los clusters √≥ptimos seg√∫n cada
m√©todo:

-   **Ward**: Sugiere 2 grupos principales basados en las alturas de
    corte del dendrograma.

-   **Average**: Identifica 3 grupos distintos con uno de ellos
    compuesto por solamente dos individuos.

# üìê An√°lisis de Componentes Principales

```{r pca-analysis}
# PCA
acp <- princomp(datos_st)
summary(acp)

# Cargas de las componentes principales
round(acp$loadings[,1:2], 3) %>%
  kable(caption = "Cargas de las dos primeras componentes principales") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

## Visualizaci√≥n con Clusters

```{r pca-visualization, fig.cap="Clustering jer√°rquico visualizado en el espacio PCA"}
# Ward con 2 clusters
datos_st$Grupo <- nbclust.ward$Best.partition

df_acp <- data.frame(
  PC1 = acp$scores[,1],
  PC2 = acp$scores[,2],
  Grupo = factor(datos_st$Grupo)
)

p1 <- ggplot(df_acp, aes(x = PC1, y = PC2, color = Grupo)) +
  geom_point(size = 1.5) +
  geom_text(aes(label = rownames(datos_st)), hjust = 0, vjust = 2, size = 1.5) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  scale_color_manual(values = c("#FF9999", "#66B2FF")) +
  labs(x = "Primera Componente Principal",
       y = "Segunda Componente Principal") +
  ggtitle("Clustering Jer√°rquico Ward")

# Average con 3 clusters
datos_st$Grupo <- nbclust.average$Best.partition

df_acp <- data.frame(
  PC1 = acp$scores[,1],
  PC2 = acp$scores[,2],
  Grupo = factor(datos_st$Grupo)
)

p2 <- ggplot(df_acp, aes(x = PC1, y = PC2, color = Grupo)) +
  geom_point(size = 1.5) +
  geom_text(aes(label = rownames(datos_st)), hjust = 0, vjust = 2, size = 1.5) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  scale_color_manual(values = c("#FF9999", "#66B2FF", "#09FA97")) +
  labs(x = "Primera Componente Principal",
       y = "Segunda Componente Principal") +
  ggtitle("Clustering Jer√°rquico Average")

grid.arrange(p1, p2, ncol = 1)
```

Los **gr√°ficos PCA** proyectan los clusters en un espacio bidimensional
donde:

-   **Eje X (CP1)**: Rendimiento acad√©mico (derecha = mejor
    rendimiento).

-   **Eje Y (CP2)**: Combinaci√≥n de condici√≥n f√≠sica y estr√©s (arriba =
    alta condici√≥n f√≠sica + alto estr√©s).

-   **Colores**: Representan los diferentes grupos identificados.

-   **L√≠neas punteadas**: Marcan los valores promedio (punto de
    referencia)

## Interpretaci√≥n de Componentes

En cuanto al gr√°fico obtenido, se puede apreciar como hay dos grupos
principales en ambos m√©todos con ciertas variaciones de individuos que
se encuentran m√°s centrados en el gr√°fico.

**Primera Componente Principal (CP1)**: Parece tener que ver con el
rendimiento acad√©mico (correlaci√≥n positiva con Promedios en
asignaturas) y buenos h√°bitos de estudio (correlaciones positivas con
Horas de sue√±o, Horas de estudio y Asistencia, correlaci√≥n negativa con
Uso de Dispositivos).

**Segunda Componente Principal (CP2)**: Propone una diferenciaci√≥n
compleja relacionada con aspectos f√≠sicos y emocionales contradictorios
donde valores altos indican simult√°neamente alta Condici√≥n F√≠sica
(positivo) y alto Nivel de Estr√©s (negativo).

# üîß An√°lisis con Outliers Eliminados

Se observa que algunos individuos at√≠picos (individuos 72 y 79)
corresponden a los valores m√°s altos en la variable Condici√≥n F√≠sica. Se
realizar√° el an√°lisis eliminando estos outliers.

```{r outliers-removed}
# Eliminaci√≥n de outliers espec√≠ficos
datos_2 <- datos_b[c(-72, -79),]
boxplot(datos_2[,10], las = 2, cex.axis = 0.7, 
        main = "Distribuci√≥n de Condici√≥n F√≠sica sin outliers")

# Nuevo an√°lisis
datos_2 <- datos_2[,-11]
datos_st_new <- as.data.frame(scale(datos_2, center = TRUE, scale = TRUE))

D_new <- dist(datos_st_new)
hc.avg_new <- hclust(D_new, method = "average")

# Evaluaci√≥n de m√©todos sin outliers
methods <- c("ward.D2", "average", "complete", "single")
c_coef_new <- sapply(methods, function(m) {
    hc <- hclust(D_new, method = m)
    cor(cophenetic(hc), D_new)
})

# Tabla de resultados ordenada de mayor a menor
resultado_metodos_new <- data.frame(
  M√©todo = names(c_coef_new), 
  Correlaci√≥n_Cofen√©tica = round(c_coef_new, 4)
) %>%
  arrange(desc(Correlaci√≥n_Cofen√©tica))

kable(resultado_metodos_new, 
      caption = "Evaluaci√≥n de m√©todos sin outliers (ordenado de mejor a peor)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  row_spec(1, bold = TRUE, color = "white", background = "#28a745")
```

El m√©todo **average** muestra ahora la mejor correlaci√≥n cofen√©tica
(0.5655), por lo que se selecciona como el m√°s adecuado.

```{r optimal-clusters-refined}
# N√∫mero √≥ptimo de clusters sin outliers
nbclust.average_new <- NbClust(data = datos_st_new, 
                              diss = NULL, 
                              distance = "euclidean", 
                              method = "average")
```

```{r final-dendrogram, fig.cap="Dendrograma final con m√©todo Average"}
plot(hc.avg_new, main = "M√©todo Average - An√°lisis Final", hang = -1)
rect.hclust(hc.avg_new, k = 3, border = "blue")
```

# üìà Resultados Finales

```{r final-pca}
# PCA del an√°lisis refinado
acp_final <- princomp(datos_st_new)
acp_final$loadings[,1] <- -1 * acp_final$loadings[,1]  # Ajuste de orientaci√≥n

# Cargas finales
round(acp_final$loadings[,1:2], 3) %>%
  kable(caption = "Cargas de componentes principales - An√°lisis final") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

```{r final-visualization, fig.cap="Segmentaci√≥n final de estudiantes"}
datos_st_new$Grupo <- nbclust.average_new$Best.partition

df_acp_final <- data.frame(
  PC1 = -1 * acp_final$scores[,1],
  PC2 = acp_final$scores[,2],
  Grupo = factor(datos_st_new$Grupo)
)

ggplot(df_acp_final, aes(x = PC1, y = PC2, color = Grupo)) +
  geom_point(size = 1.5) +
  geom_text(aes(label = rownames(datos_st_new)), hjust = 0, vjust = 2, size = 1.5) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  scale_color_manual(values = c("#FF9999", "#66B2FF", "#09FA97")) +
  labs(x = "Primera Componente Principal",
       y = "Segunda Componente Principal") +
  ggtitle("Clustering Jer√°rquico Average - An√°lisis Final")
```

El **gr√°fico final** muestra la segmentaci√≥n definitiva donde se observa
claramente:

-   **Grupo 1 (Rojo)**: Concentrado en la zona izquierda (bajo
    rendimiento acad√©mico).

-   **Grupo 2 (Azul)**: Disperso en el centro con tendencia variable en
    CP2 (rendimiento medio).

-   **Grupo 3 (Verde)**: Ubicado en la zona derecha con valores diversos
    en CP2 (alto rendimiento acad√©mico)

# üë• Caracterizaci√≥n de Grupos Identificados

En cuanto a los grupos obtenidos, se puede observar como estos se
dividen seg√∫n el rendimiento acad√©mico (CP1) principalmente.

```{r group-summary}
# Resumen por grupos
grupo_summary <- datos_st_new %>%
  group_by(Grupo) %>%
  summarise(
    N_estudiantes = n(),
    across(everything(), ~ round(mean(.x), 2)),
    .groups = 'drop'
  )

kable(grupo_summary, 
      caption = "Caracter√≠sticas promedio por grupo (valores estandarizados)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  row_spec(1, background = "#FFE6E6") %>%  # Rojo claro para Grupo 1
  row_spec(2, background = "#E6F3FF") %>%  # Azul claro para Grupo 2  
  row_spec(3, background = "#E6FFE6")     # Verde claro para Grupo 3
```

```{r group-characteristics, fig.cap="Perfil de caracter√≠sticas por grupo"}
# Gr√°fico de radar/perfil por grupo

# Seleccionar variables clave para caracterizaci√≥n
vars_clave <- c("Promedio_matematicas", "Promedio_ciencias", "Promedio_lectura", 
                "Horas_estudio", "Uso_dispositivos", "Asistencia", "Condicion_fisica", "Nivel_estres")

perfil_grupos <- datos_st_new %>%
  select(Grupo, all_of(vars_clave)) %>%
  group_by(Grupo) %>%
  summarise(across(everything(), mean), .groups = 'drop') %>%
  melt(id.vars = "Grupo", variable.name = "Variable", value.name = "Valor")

# Gr√°fico de barras agrupadas
ggplot(perfil_grupos, aes(x = Variable, y = Valor, fill = factor(Grupo))) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.8) +
  scale_fill_manual(values = c("1" = "#FF9999", "2" = "#66B2FF", "3" = "#09FA97"),
                    name = "Grupo") +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Perfil de Caracter√≠sticas por Grupo",
       subtitle = "Valores estandarizados (0 = media poblacional)",
       x = "Variables", y = "Valor Estandarizado") +
  coord_cartesian(ylim = c(-1.5, 1.5))
```

El **gr√°fico de barras** compara los grupos en variables clave donde:

-   **L√≠nea horizontal (0)**: Representa la media de toda la poblaci√≥n.

-   **Barras positivas**: Por encima de la media poblacional.

-   **Barras negativas**: Por debajo de la media poblacional.

Se observa claramente como el **Grupo 3 (verde)** supera la media en
rendimiento acad√©mico y de buenos h√°bitos (), mientras que el **Grupo 1
(rojo)** est√° consistentemente por debajo. Adem√°s de esto se ven las
diferencias notables respecto al estr√©s y la condici√≥n f√≠sica donde el
Grupo 3 est√° por encima de la media.

## üî¥ Grupo 1: Estudiantes con Bajo Rendimiento

**Caracter√≠sticas principales:**

-   üìâ **Rendimiento acad√©mico**: Por debajo de la media en todas las
    asignaturas.

-   üìö **H√°bitos de estudio**: Menor dedicac√≥n en horas de estudio y
    asistencia a clases, a la vez que mayor uso de dispositivos
    electr√≥nicos junto con menor horas de sue√±o.

-   üéØ **Patr√≥n identificativo**: Valores negativos consistentes en CP1
    (rendimiento acad√©mico).

-   üìä **Dispersi√≥n**: Comportamiento variable en CP2 (aspectos
    f√≠sicos/emocionales)

------------------------------------------------------------------------

## üîµ Grupo 2: Estudiantes con Rendimiento Medio

**Caracter√≠sticas principales:**

-   üìä **Rendimiento acad√©mico**: Valores medios o ligeramente por
    debajo de la media.

-   ‚öñÔ∏è **Patr√≥n compensatorio**: Si tienen valores altos en rendimiento,
    presentan patrones variables en aspectos f√≠sicos/emocionales.

-   üéØ **Patr√≥n identificativo**: Posici√≥n central en CP1, tendencia
    ligeramente negativa en CP2.

-   üîÑ **Equilibrio inestable**: Compensaci√≥n entre diferentes aspectos
    del desarrollo estudiantil

------------------------------------------------------------------------

## üü¢ Grupo 3: Estudiantes de Alto Rendimiento Integral

**Caracter√≠sticas principales:**

-   üåü **Rendimiento acad√©mico**: Bastante por encima de la media en
    todas las √°reas.

-   üí™ **Perfil complejo**: Buen rendimiento acad√©mico con patrones
    diversos en condici√≥n f√≠sica y estr√©s.

-   **üéØ Patr√≥n identificativo**: Valores positivos en CP1 con tendencia
    positiva en CP2.

-   üèÜ **Excelencia acad√©mica**: Destacan principalmente en rendimiento
    acad√©mico.

```{r classification-table}
# Tabla de criterios de clasificaci√≥n
criterios_clasificacion <- data.frame(
  Criterio = c("Rendimiento Acad√©mico/H√°bitos (CP1)", "Condici√≥n F√≠sica/Estr√©s (CP2)", "Posici√≥n en Gr√°fico PCA"),
  "Grupo 1" = c("< -0.5 (Bajo)", "Variable", "Lado izquierdo"),
  "Grupo 2" = c("-0.5 a 0.5 (Medio)", "Tendencia ligeramente negativa", "Centro"),
  "Grupo 3" = c("> 0.5 (Alto)", "Tendencia positiva", "Lado derecho"),
  check.names = FALSE
)

kable(criterios_clasificacion, 
      caption = "Criterios de Clasificaci√≥n de Estudiantes",
      align = "lccc") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(2, background = "#FFE6E6") %>%  # Grupo 1 - rojo claro
  column_spec(3, background = "#E6F3FF") %>%  # Grupo 2 - azul claro
  column_spec(4, background = "#E6FFE6")     # Verde claro para Grupo 3
```

## Resumen de Identificaci√≥n

La clasificaci√≥n de estudiantes se basa principalmente en la **Primera
Componente Principal (CP1)**, que representa el rendimiento acad√©mico
general, complementada por la **Segunda Componente Principal (CP2)**,
que refleja una combinaci√≥n compleja de condici√≥n f√≠sica y nivel de
estr√©s.

**Regla de clasificaci√≥n simplificada:**

1.  **Calcular CP1** (combinaci√≥n de promedios acad√©micos y h√°bitos de
    estudio y de bienestar)
2.  **Evaluar CP2** (equilibrio entre condici√≥n f√≠sica y nivel de
    estr√©s)
3.  **Aplicar umbrales** en el espacio bidimensional PCA
4.  **Asignar grupo** seg√∫n la posici√≥n en el gr√°fico de componentes
    principales

# üìã Conclusiones

El an√°lisis de clustering ha permitido identificar **tres grupos
distintos de estudiantes** con caracter√≠sticas diferenciadas en t√©rminos
de rendimiento acad√©mico y patrones f√≠sicos/emocionales.

## Hallazgos Principales

‚úÖ **Eliminaci√≥n exitosa de outliers**: Los estudiantes 72 y 79 (valores
extremos en Condici√≥n F√≠sica) fueron correctamente identificados y
eliminados, mejorando la calidad del clustering.

‚úÖ **M√©todo √≥ptimo identificado**: El m√©todo Average con correlaci√≥n
cofen√©tica de 0.5655 result√≥ ser el m√°s adecuado tras la eliminaci√≥n de
outliers.

‚úÖ **Tres perfiles claramente diferenciados**: La segmentaci√≥n revel√≥
grupos con caracter√≠sticas distintivas y interpretables.

‚úÖ **Componentes principales interpretables**:

-   CP1 representa el rendimiento acad√©mico general y h√°bitos
    estudiantiles y de bienestar.

-   CP2 refleja una combinaci√≥n compleja de condici√≥n f√≠sica y estr√©s
