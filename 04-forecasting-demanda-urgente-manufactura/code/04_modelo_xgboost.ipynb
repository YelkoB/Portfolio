{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: Modelo XGBoost para Predicci√≥n de Urgencias\n",
    "\n",
    "**Objetivo**: Entrenar clasificador XGBoost para predecir urgencias (Definici√≥n A: P75) usando features de series temporales.\n",
    "\n",
    "## Estrategia:\n",
    "1. Cargar datasets con features (train/test)\n",
    "2. Entrenar XGBoost con class_weight para manejar desbalance\n",
    "3. Evaluar con m√©tricas: Accuracy, Precision, Recall, F1, ROC-AUC\n",
    "4. Analizar feature importance\n",
    "5. Generar predicciones y an√°lisis por producto\n",
    "6. Guardar modelo y resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc\n",
    ")\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úì Librer√≠as cargadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar train/test con features\n",
    "df_train = pd.read_csv('../data/simulated/train_features.csv')\n",
    "df_test = pd.read_csv('../data/simulated/test_features.csv')\n",
    "\n",
    "# Cargar lista de features\n",
    "features_list = pd.read_csv('../data/simulated/feature_list.csv')['feature'].tolist()\n",
    "\n",
    "print(f\"üìä Datos cargados:\")\n",
    "print(f\"\\nTRAIN:\")\n",
    "print(f\"  ‚Ä¢ Registros: {len(df_train):,}\")\n",
    "print(f\"  ‚Ä¢ Productos: {df_train['item_id'].nunique():,}\")\n",
    "print(f\"  ‚Ä¢ Urgencias: {df_train['is_urgent_a'].sum():,} ({df_train['is_urgent_a'].mean():.1%})\")\n",
    "\n",
    "print(f\"\\nTEST:\")\n",
    "print(f\"  ‚Ä¢ Registros: {len(df_test):,}\")\n",
    "print(f\"  ‚Ä¢ Productos: {df_test['item_id'].nunique():,}\")\n",
    "print(f\"  ‚Ä¢ Urgencias: {df_test['is_urgent_a'].sum():,} ({df_test['is_urgent_a'].mean():.1%})\")\n",
    "\n",
    "print(f\"\\n‚úì Features disponibles: {len(features_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparaci√≥n de Features y Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar features y target\n",
    "X_train = df_train[features_list]\n",
    "y_train = df_train['is_urgent_a']\n",
    "\n",
    "X_test = df_test[features_list]\n",
    "y_test = df_test['is_urgent_a']\n",
    "\n",
    "print(f\"‚úì Shapes preparados:\")\n",
    "print(f\"  ‚Ä¢ X_train: {X_train.shape}\")\n",
    "print(f\"  ‚Ä¢ y_train: {y_train.shape}\")\n",
    "print(f\"  ‚Ä¢ X_test: {X_test.shape}\")\n",
    "print(f\"  ‚Ä¢ y_test: {y_test.shape}\")\n",
    "\n",
    "# Verificar valores infinitos o NaN\n",
    "print(f\"\\nüîç Verificaci√≥n de calidad:\")\n",
    "print(f\"  ‚Ä¢ NaN en X_train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"  ‚Ä¢ Inf en X_train: {np.isinf(X_train.values).sum()}\")\n",
    "print(f\"  ‚Ä¢ NaN en X_test: {X_test.isnull().sum().sum()}\")\n",
    "print(f\"  ‚Ä¢ Inf en X_test: {np.isinf(X_test.values).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular scale_pos_weight para manejar desbalance\n",
    "# Ratio de negativos/positivos\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "print(f\"‚öñÔ∏è Class balance:\")\n",
    "print(f\"  ‚Ä¢ Clase 0 (no urgente): {(y_train == 0).sum():,} ({(y_train == 0).mean():.1%})\")\n",
    "print(f\"  ‚Ä¢ Clase 1 (urgente): {(y_train == 1).sum():,} ({(y_train == 1).mean():.1%})\")\n",
    "print(f\"  ‚Ä¢ scale_pos_weight: {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar modelo XGBoost\n",
    "print(\"üöÄ Entrenando XGBoost...\\n\")\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=200,           # N√∫mero de √°rboles\n",
    "    max_depth=6,                # Profundidad m√°xima\n",
    "    learning_rate=0.1,          # Tasa de aprendizaje\n",
    "    subsample=0.8,              # Fracci√≥n de muestras por √°rbol\n",
    "    colsample_bytree=0.8,       # Fracci√≥n de features por √°rbol\n",
    "    scale_pos_weight=scale_pos_weight,  # Balance de clases\n",
    "    objective='binary:logistic', # Clasificaci√≥n binaria\n",
    "    eval_metric='logloss',      # M√©trica de evaluaci√≥n\n",
    "    random_state=42,\n",
    "    n_jobs=-1                   # Usar todos los cores\n",
    ")\n",
    "\n",
    "# Entrenar con early stopping\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=50  # Mostrar progreso cada 50 iteraciones\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Modelo entrenado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones en train y test\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"‚úì Predicciones generadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluaci√≥n del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©tricas en TRAIN\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "train_auc = roc_auc_score(y_train, y_train_proba)\n",
    "\n",
    "# M√©tricas en TEST\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä M√âTRICAS DE EVALUACI√ìN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'M√©trica':<20} {'TRAIN':<15} {'TEST':<15} {'Diferencia':<15}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Accuracy':<20} {train_acc:>14.3f} {test_acc:>14.3f} {train_acc - test_acc:>14.3f}\")\n",
    "print(f\"{'Precision':<20} {train_precision:>14.3f} {test_precision:>14.3f} {train_precision - test_precision:>14.3f}\")\n",
    "print(f\"{'Recall':<20} {train_recall:>14.3f} {test_recall:>14.3f} {train_recall - test_recall:>14.3f}\")\n",
    "print(f\"{'F1-Score':<20} {train_f1:>14.3f} {test_f1:>14.3f} {train_f1 - test_f1:>14.3f}\")\n",
    "print(f\"{'ROC-AUC':<20} {train_auc:>14.3f} {test_auc:>14.3f} {train_auc - test_auc:>14.3f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Interpretaci√≥n\n",
    "print(f\"\\nüéØ INTERPRETACI√ìN:\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {test_acc:.1%} de predicciones correctas\")\n",
    "print(f\"  ‚Ä¢ Precision: {test_precision:.1%} de las urgencias predichas son reales\")\n",
    "print(f\"  ‚Ä¢ Recall: {test_recall:.1%} de las urgencias reales fueron detectadas\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {test_f1:.3f} (balance entre precision y recall)\")\n",
    "print(f\"  ‚Ä¢ ROC-AUC: {test_auc:.3f} (capacidad discriminativa del modelo)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report detallado\n",
    "print(\"\\nüìã CLASSIFICATION REPORT (TEST):\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_test, y_test_pred, target_names=['No Urgente', 'Urgente']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Urgente', 'Urgente'],\n",
    "            yticklabels=['No Urgente', 'Urgente'],\n",
    "            ax=ax, cbar_kws={'label': 'Frecuencia'})\n",
    "\n",
    "ax.set_xlabel('Predicci√≥n', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Real', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Matriz de Confusi√≥n - XGBoost (Test Set)', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# A√±adir totales\n",
    "total = cm.sum()\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "ax.text(2.3, 0.3, f'TN: {tn}\\n({tn/total:.1%})', fontsize=10, ha='left')\n",
    "ax.text(2.3, 1.3, f'TP: {tp}\\n({tp/total:.1%})', fontsize=10, ha='left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/confusion_matrix_xgboost.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Gr√°fico guardado: results/confusion_matrix_xgboost.png\")\n",
    "print(f\"\\nDesglose:\")\n",
    "print(f\"  ‚Ä¢ True Negatives (TN): {tn:,} - Correctamente identificados como no urgentes\")\n",
    "print(f\"  ‚Ä¢ False Positives (FP): {fp:,} - Falsa alarma (predijo urgente, no lo era)\")\n",
    "print(f\"  ‚Ä¢ False Negatives (FN): {fn:,} - Urgencia perdida (no detect√≥ urgencia real)\")\n",
    "print(f\"  ‚Ä¢ True Positives (TP): {tp:,} - Urgencia correctamente detectada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Curva ROC\n",
    "ax.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "        label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# L√≠nea diagonal (clasificador aleatorio)\n",
    "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "        label='Random classifier (AUC = 0.500)')\n",
    "\n",
    "# Punto √≥ptimo (m√°ximo Youden's J)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "ax.plot(fpr[optimal_idx], tpr[optimal_idx], 'ro', markersize=10, \n",
    "        label=f'Optimal threshold = {optimal_threshold:.3f}')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC Curve - XGBoost', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/roc_curve_xgboost.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Gr√°fico guardado: results/roc_curve_xgboost.png\")\n",
    "print(f\"\\nUmbral √≥ptimo: {optimal_threshold:.3f}\")\n",
    "print(f\"  ‚Ä¢ TPR (Recall): {tpr[optimal_idx]:.3f}\")\n",
    "print(f\"  ‚Ä¢ FPR: {fpr[optimal_idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features_list,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä TOP 20 FEATURES M√ÅS IMPORTANTES:\")\n",
    "print(\"=\"*60)\n",
    "for idx, row in feature_importance.head(20).iterrows():\n",
    "    print(f\"{row['feature']:<30} {row['importance']:>10.4f}\")\n",
    "\n",
    "# Guardar feature importance completa\n",
    "feature_importance.to_csv('../results/feature_importance_xgboost.csv', index=False)\n",
    "print(f\"\\n‚úì Feature importance guardada: results/feature_importance_xgboost.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar top 20 features\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "\n",
    "ax.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'])\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 20 Features - XGBoost', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/feature_importance_xgboost.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gr√°fico guardado: results/feature_importance_xgboost.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. An√°lisis por Producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar predicciones al dataframe de test\n",
    "df_test['pred_urgent'] = y_test_pred\n",
    "df_test['pred_proba'] = y_test_proba\n",
    "\n",
    "# M√©tricas por producto\n",
    "metricas_producto = df_test.groupby('item_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'n_semanas': len(x),\n",
    "        'urgencias_reales': x['is_urgent_a'].sum(),\n",
    "        'urgencias_pred': x['pred_urgent'].sum(),\n",
    "        'accuracy': accuracy_score(x['is_urgent_a'], x['pred_urgent']),\n",
    "        'precision': precision_score(x['is_urgent_a'], x['pred_urgent'], zero_division=0),\n",
    "        'recall': recall_score(x['is_urgent_a'], x['pred_urgent'], zero_division=0),\n",
    "        'f1': f1_score(x['is_urgent_a'], x['pred_urgent'], zero_division=0)\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\nüì¶ M√âTRICAS POR PRODUCTO (Estad√≠sticas):\")\n",
    "print(\"=\"*70)\n",
    "print(metricas_producto[['accuracy', 'precision', 'recall', 'f1']].describe())\n",
    "\n",
    "# Top 10 productos mejor predichos (mayor F1)\n",
    "print(\"\\nüèÜ TOP 10 PRODUCTOS MEJOR PREDICHOS (Mayor F1):\")\n",
    "print(\"=\"*70)\n",
    "top_productos = metricas_producto.nlargest(10, 'f1')\n",
    "for idx, row in top_productos.iterrows():\n",
    "    print(f\"{row['item_id']:<20} F1: {row['f1']:.3f}  Accuracy: {row['accuracy']:.3f}  Recall: {row['recall']:.3f}\")\n",
    "\n",
    "# Guardar m√©tricas por producto\n",
    "metricas_producto.to_csv('../results/metricas_por_producto_xgboost.csv', index=False)\n",
    "print(f\"\\n‚úì M√©tricas por producto guardadas: results/metricas_por_producto_xgboost.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Guardar Modelo y Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo entrenado\n",
    "joblib.dump(model, '../models/xgboost_urgency_classifier.pkl')\n",
    "print(\"‚úì Modelo guardado: models/xgboost_urgency_classifier.pkl\")\n",
    "\n",
    "# Guardar predicciones de test\n",
    "predicciones_test = df_test[['item_id', 'week_start', 'total_sales', \n",
    "                              'is_urgent_a', 'pred_urgent', 'pred_proba']].copy()\n",
    "predicciones_test.to_csv('../results/predicciones_test_xgboost.csv', index=False)\n",
    "print(\"‚úì Predicciones guardadas: results/predicciones_test_xgboost.csv\")\n",
    "\n",
    "# Guardar m√©tricas resumen\n",
    "metricas_resumen = pd.DataFrame({\n",
    "    'modelo': ['XGBoost'],\n",
    "    'train_accuracy': [train_acc],\n",
    "    'test_accuracy': [test_acc],\n",
    "    'train_precision': [train_precision],\n",
    "    'test_precision': [test_precision],\n",
    "    'train_recall': [train_recall],\n",
    "    'test_recall': [test_recall],\n",
    "    'train_f1': [train_f1],\n",
    "    'test_f1': [test_f1],\n",
    "    'train_auc': [train_auc],\n",
    "    'test_auc': [test_auc],\n",
    "    'n_features': [len(features_list)],\n",
    "    'n_train': [len(df_train)],\n",
    "    'n_test': [len(df_test)]\n",
    "})\n",
    "\n",
    "metricas_resumen.to_csv('../results/metricas_resumen_xgboost.csv', index=False)\n",
    "print(\"‚úì M√©tricas resumen guardadas: results/metricas_resumen_xgboost.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ RESUMEN MODELO XGBOOST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä RENDIMIENTO EN TEST SET:\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {test_acc:.1%}\")\n",
    "print(f\"  ‚Ä¢ Precision: {test_precision:.1%}\")\n",
    "print(f\"  ‚Ä¢ Recall: {test_recall:.1%}\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {test_f1:.3f}\")\n",
    "print(f\"  ‚Ä¢ ROC-AUC: {test_auc:.3f}\")\n",
    "\n",
    "print(f\"\\nüîç TOP 5 FEATURES M√ÅS IMPORTANTES:\")\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  {idx+1}. {row['feature']:<25} (importance: {row['importance']:.4f})\")\n",
    "\n",
    "print(f\"\\nüì¶ COBERTURA:\")\n",
    "print(f\"  ‚Ä¢ Productos evaluados: {df_test['item_id'].nunique():,}\")\n",
    "print(f\"  ‚Ä¢ Semanas de predicci√≥n: {df_test['week_start'].nunique()}\")\n",
    "print(f\"  ‚Ä¢ Total predicciones: {len(df_test):,}\")\n",
    "\n",
    "print(f\"\\nüíæ ARCHIVOS GENERADOS:\")\n",
    "print(f\"  ‚Ä¢ models/xgboost_urgency_classifier.pkl\")\n",
    "print(f\"  ‚Ä¢ results/confusion_matrix_xgboost.png\")\n",
    "print(f\"  ‚Ä¢ results/roc_curve_xgboost.png\")\n",
    "print(f\"  ‚Ä¢ results/feature_importance_xgboost.png\")\n",
    "print(f\"  ‚Ä¢ results/feature_importance_xgboost.csv\")\n",
    "print(f\"  ‚Ä¢ results/predicciones_test_xgboost.csv\")\n",
    "print(f\"  ‚Ä¢ results/metricas_por_producto_xgboost.csv\")\n",
    "print(f\"  ‚Ä¢ results/metricas_resumen_xgboost.csv\")\n",
    "\n",
    "print(f\"\\n‚úÖ MODELO XGBOOST COMPLETADO\")\n",
    "print(f\"\\nPr√≥ximos pasos:\")\n",
    "print(f\"  ‚Üí Notebook 05: Modelo Prophet (series temporales)\")\n",
    "print(f\"  ‚Üí Notebook 06: Modelo Random Forest\")\n",
    "print(f\"  ‚Üí Notebook 07: Comparaci√≥n de modelos\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
