{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Feature Engineering\n",
    "\n",
    "**Objetivo**: Crear features temporales, de rezago y estad√≠sticas para modelado predictivo de urgencias.\n",
    "\n",
    "## Estrategia:\n",
    "1. Cargar datos de urgencias redefinidas (Definici√≥n A: Percentil 75)\n",
    "2. Filtrar productos con predictibilidad Moderada/Alta\n",
    "3. Crear features:\n",
    "   - **Temporales**: semana, mes, trimestre, estacionalidad\n",
    "   - **Lags**: rezagos de ventas (1, 2, 4, 8, 52 semanas)\n",
    "   - **Rolling stats**: MA, std, min, max en ventanas m√≥viles\n",
    "   - **Urgency-specific**: d√≠as desde √∫ltima urgencia, frecuencia\n",
    "4. Preparar train/test split temporal\n",
    "5. Guardar dataset listo para modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úì Librer√≠as cargadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cargar urgencias redefinidas\ndf_urgencias = pd.read_csv('../data/simulated/urgencias_redefinidas.csv')\ndf_urgencias['week_start'] = pd.to_datetime(df_urgencias['week_start'])\n\n# Cargar clasificaci√≥n de productos\ndf_productos = pd.read_csv('../data/simulated/productos_predecibles.csv')\n\nprint(f\"Datos cargados:\")\nprint(f\"  ‚Ä¢ Registros totales: {len(df_urgencias):,}\")\nprint(f\"  ‚Ä¢ Productos √∫nicos: {df_urgencias['item_id'].nunique():,}\")\nprint(f\"  ‚Ä¢ Rango temporal: {df_urgencias['week_start'].min()} a {df_urgencias['week_start'].max()}\")\nprint(f\"\\nDistribuci√≥n de predictibilidad:\")\nprint(df_productos['clasificacion'].value_counts())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filtrar solo productos Predecibles y Moderados\nproductos_modelar = df_productos[\n    df_productos['clasificacion'].isin(['Predecible', 'Moderado'])\n]['item_id'].unique()\n\ndf = df_urgencias[df_urgencias['item_id'].isin(productos_modelar)].copy()\ndf = df.sort_values(['item_id', 'week_start']).reset_index(drop=True)\n\nprint(f\"\\n‚úì Dataset filtrado:\")\nprint(f\"  ‚Ä¢ Productos a modelar: {len(productos_modelar):,}\")\nprint(f\"  ‚Ä¢ Registros: {len(df):,}\")\nprint(f\"  ‚Ä¢ Proporci√≥n urgencias: {df['is_urgent_a'].mean():.1%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Features Temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extraer componentes temporales\ndf['year'] = df['week_start'].dt.year\ndf['month'] = df['week_start'].dt.month\ndf['quarter'] = df['week_start'].dt.quarter\ndf['week_of_year'] = df['week_start'].dt.isocalendar().week\ndf['day_of_year'] = df['week_start'].dt.dayofyear\n\n# Features c√≠clicas (sin/cos para capturar periodicidad)\ndf['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\ndf['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\ndf['week_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\ndf['week_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\n\n# Indicadores especiales\ndf['is_q4'] = (df['quarter'] == 4).astype(int)  # Q4 suele tener alta demanda\ndf['is_january'] = (df['month'] == 1).astype(int)  # Enero post-navidad\n\nprint(\"‚úì Features temporales creadas\")\nprint(f\"  ‚Ä¢ Columnas temporales: {['year', 'month', 'quarter', 'week_of_year', 'month_sin', 'month_cos', 'week_sin', 'week_cos']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lag Features (Rezagos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear lags de ventas por producto\n",
    "lag_periods = [1, 2, 4, 8, 52]  # 1w, 2w, 1m, 2m, 1y\n",
    "\n",
    "for lag in lag_periods:\n",
    "    df[f'sales_lag_{lag}'] = df.groupby('item_id')['total_sales'].shift(lag)\n",
    "    print(f\"  ‚Ä¢ Lag {lag} semanas creado\")\n",
    "\n",
    "# Lags de urgencias (binario)\n",
    "for lag in [1, 2, 4]:\n",
    "    df[f'urgent_lag_{lag}'] = df.groupby('item_id')['is_urgent_a'].shift(lag)\n",
    "\n",
    "print(\"\\n‚úì Lag features creadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rolling Statistics (Ventanas M√≥viles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Media m√≥vil y estad√≠sticas en ventanas de 4, 8, 12 semanas\n# Nota: ma_4 ya existe del notebook 02, por lo que creamos nuevas columnas con prefijo 'roll_'\nwindows = [4, 8, 12]\n\nfor window in windows:\n    # Media m√≥vil\n    df[f'roll_ma_{window}'] = df.groupby('item_id')['total_sales'].transform(\n        lambda x: x.rolling(window=window, min_periods=1).mean()\n    )\n    \n    # Desviaci√≥n est√°ndar m√≥vil\n    df[f'roll_std_{window}'] = df.groupby('item_id')['total_sales'].transform(\n        lambda x: x.rolling(window=window, min_periods=1).std()\n    )\n    \n    # M√≠nimo y m√°ximo m√≥vil\n    df[f'roll_min_{window}'] = df.groupby('item_id')['total_sales'].transform(\n        lambda x: x.rolling(window=window, min_periods=1).min()\n    )\n    \n    df[f'roll_max_{window}'] = df.groupby('item_id')['total_sales'].transform(\n        lambda x: x.rolling(window=window, min_periods=1).max()\n    )\n    \n    print(f\"  ‚Ä¢ Rolling stats ventana {window} semanas\")\n\n# Features derivadas - usar la ma_4 existente del notebook 02\ndf['sales_vs_ma4'] = df['total_sales'] / (df['ma_4'] + 1)  # Ratio vs tendencia\ndf['sales_vs_roll_ma12'] = df['total_sales'] / (df['roll_ma_12'] + 1)\n\nprint(\"\\n‚úì Rolling statistics creadas\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Features Espec√≠ficas de Urgencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def calcular_dias_desde_ultima_urgencia(grupo):\n    \"\"\"Calcula d√≠as desde la √∫ltima urgencia para cada producto\n    IMPORTANTE: Usa solo informaci√≥n PASADA (excluye la semana actual)\n    \"\"\"\n    dias = []\n    ultimo_urgente = None\n    \n    for idx, row in grupo.iterrows():\n        if ultimo_urgente is None:\n            dias.append(0)\n        else:\n            dias.append((row['week_start'] - ultimo_urgente).days)\n        \n        # CLAVE: Actualizar ultimo_urgente DESPU√âS de calcular d√≠as\n        # Esto asegura que no usamos info de la semana actual para predecir\n        # Solo vemos si la ANTERIOR fue urgente\n        if idx > 0:  # Empezar desde la segunda fila\n            prev_urgent = grupo.iloc[grupo.index.get_loc(idx) - 1]['is_urgent_a'] if grupo.index.get_loc(idx) > 0 else 0\n            if prev_urgent == 1:\n                ultimo_urgente = grupo.iloc[grupo.index.get_loc(idx) - 1]['week_start']\n    \n    return pd.Series(dias, index=grupo.index)\n\n# D√≠as desde √∫ltima urgencia (usando solo info pasada)\ndf['days_since_urgent'] = df.groupby('item_id').apply(calcular_dias_desde_ultima_urgencia).values\n\n# CORRECCI√ìN CR√çTICA: Frecuencia de urgencias en ventanas m√≥viles\n# Usar shift(1) para excluir la semana actual y evitar data leakage\nfor window in [4, 8, 12]:\n    # Calcular rolling sobre urgencias PASADAS (shift 1 posici√≥n)\n    df[f'urgent_freq_{window}w'] = df.groupby('item_id')['is_urgent_a'].shift(1).transform(\n        lambda x: x.rolling(window=window, min_periods=1).sum()\n    )\n    \n    # Tasa de urgencias (proporci√≥n)\n    df[f'urgent_rate_{window}w'] = df[f'urgent_freq_{window}w'] / window\n\nprint(\"‚úì Features de urgencias creadas (SIN data leakage)\")\nprint(f\"  ‚Ä¢ days_since_urgent: promedio {df['days_since_urgent'].mean():.1f} d√≠as\")\nprint(f\"  ‚Ä¢ urgent_rate_4w: promedio {df['urgent_rate_4w'].mean():.2%}\")\nprint(f\"\\\\n‚ö†Ô∏è  IMPORTANTE: Features usan solo informaci√≥n PASADA (shift aplicado)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Features de Tendencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Diferencias (cambio respecto a semana anterior)\ndf['sales_diff_1'] = df.groupby('item_id')['total_sales'].diff(1)\ndf['sales_diff_4'] = df.groupby('item_id')['total_sales'].diff(4)\n\n# Tasa de crecimiento\ndf['sales_pct_change_1'] = df.groupby('item_id')['total_sales'].pct_change(1)\ndf['sales_pct_change_4'] = df.groupby('item_id')['total_sales'].pct_change(4)\n\n# Momentum (diferencia entre MA corta y larga) - usar roll_ma ya que ma_4 viene del notebook 02\ndf['momentum'] = df['roll_ma_4'] - df['roll_ma_12']\n\nprint(\"‚úì Features de tendencia creadas\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Limpieza y Preparaci√≥n Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Revisar valores faltantes\nprint(\"Missing values por columna:\")\nmissing = df.isnull().sum()\nmissing = missing[missing > 0].sort_values(ascending=False)\nprint(missing.head(10))\n\n# Rellenar missing values\n# Para lags: forward fill (usamos el valor anterior)\nlag_cols = [col for col in df.columns if 'lag_' in col or 'roll_std_' in col]\ndf[lag_cols] = df.groupby('item_id')[lag_cols].ffill()\n\n# Para diferencias y pct_change: rellenar con 0\ndiff_cols = [col for col in df.columns if 'diff' in col or 'pct_change' in col]\ndf[diff_cols] = df[diff_cols].fillna(0)\n\n# Reemplazar infinitos por NaN y luego rellenar\ndf = df.replace([np.inf, -np.inf], np.nan)\ndf = df.fillna(0)\n\nprint(f\"\\n‚úì Missing values manejados\")\nprint(f\"  ‚Ä¢ Nulls restantes: {df.isnull().sum().sum()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. An√°lisis de Correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Seleccionar features num√©ricas relevantes\nfeature_cols = [col for col in df.columns if col not in [\n    'item_id', 'week_start', 'week_id', 'week_num', 'total_sales', \n    'is_urgent_a', 'is_urgent_b', 'is_urgent_c',\n    'threshold_a', 'threshold_b', 'threshold_c', 'mean_prod', 'std_prod',\n    'category', 'dept'\n]]\n\n# Calcular correlaci√≥n con variable objetivo\ncorrelations = df[feature_cols + ['is_urgent_a']].corr()['is_urgent_a'].drop('is_urgent_a')\ncorrelations = correlations.sort_values(ascending=False)\n\nprint(\"\\nüìä TOP 15 FEATURES M√ÅS CORRELACIONADAS CON URGENCIA:\")\nprint(correlations.head(15))\n\nprint(\"\\nüìä BOTTOM 10 FEATURES (menor correlaci√≥n):\")\nprint(correlations.tail(10))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar top correlaciones\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_corr = correlations.head(20)\n",
    "top_corr.plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_xlabel('Correlaci√≥n con is_urgent_a')\n",
    "ax.set_title('Top 20 Features por Correlaci√≥n con Urgencias')\n",
    "ax.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/feature_correlations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gr√°fico guardado: results/feature_correlations.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train/Test Split Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split temporal: √∫ltimas 26 semanas (6 meses) para test\nfechas_unicas = sorted(df['week_start'].unique())\nfecha_split = fechas_unicas[-26]  # √öltimas 26 semanas para test\n\ndf_train = df[df['week_start'] < fecha_split].copy()\ndf_test = df[df['week_start'] >= fecha_split].copy()\n\nprint(f\"\\nüìÖ SPLIT TEMPORAL:\")\nprint(f\"\\nTRAIN:\")\nprint(f\"  ‚Ä¢ Fechas: {df_train['week_start'].min()} a {df_train['week_start'].max()}\")\nprint(f\"  ‚Ä¢ Semanas: {df_train['week_start'].nunique()}\")\nprint(f\"  ‚Ä¢ Registros: {len(df_train):,}\")\nprint(f\"  ‚Ä¢ Urgencias: {df_train['is_urgent_a'].sum():,} ({df_train['is_urgent_a'].mean():.1%})\")\n\nprint(f\"\\nTEST:\")\nprint(f\"  ‚Ä¢ Fechas: {df_test['week_start'].min()} a {df_test['week_start'].max()}\")\nprint(f\"  ‚Ä¢ Semanas: {df_test['week_start'].nunique()}\")\nprint(f\"  ‚Ä¢ Registros: {len(df_test):,}\")\nprint(f\"  ‚Ä¢ Urgencias: {df_test['is_urgent_a'].sum():,} ({df_test['is_urgent_a'].mean():.1%})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Guardar Datasets Preparados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar dataset completo con features\n",
    "df.to_csv('../data/simulated/dataset_features.csv', index=False)\n",
    "print(f\"‚úì Dataset completo guardado: data/simulated/dataset_features.csv\")\n",
    "print(f\"  ‚Ä¢ Shape: {df.shape}\")\n",
    "\n",
    "# Guardar train/test\n",
    "df_train.to_csv('../data/simulated/train_features.csv', index=False)\n",
    "df_test.to_csv('../data/simulated/test_features.csv', index=False)\n",
    "print(f\"\\n‚úì Train/Test guardados:\")\n",
    "print(f\"  ‚Ä¢ train_features.csv: {df_train.shape}\")\n",
    "print(f\"  ‚Ä¢ test_features.csv: {df_test.shape}\")\n",
    "\n",
    "# Guardar lista de features para modelado\n",
    "features_modeling = [col for col in feature_cols if col in df.columns]\n",
    "pd.DataFrame({'feature': features_modeling}).to_csv('../data/simulated/feature_list.csv', index=False)\n",
    "print(f\"\\n‚úì Lista de features guardada: {len(features_modeling)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"üìä RESUMEN FEATURE ENGINEERING\")\nprint(\"=\"*70)\n\nprint(f\"\\n‚úì Productos procesados: {df['item_id'].nunique():,}\")\nprint(f\"‚úì Registros totales: {len(df):,}\")\nprint(f\"‚úì Features creadas: {len(feature_cols)}\")\n\nprint(f\"\\nüìÇ ARCHIVOS GENERADOS:\")\nprint(f\"  ‚Ä¢ dataset_features.csv - Dataset completo con todas las features\")\nprint(f\"  ‚Ä¢ train_features.csv - Training set ({len(df_train):,} registros)\")\nprint(f\"  ‚Ä¢ test_features.csv - Test set ({len(df_test):,} registros)\")\nprint(f\"  ‚Ä¢ feature_list.csv - Lista de features para modelado\")\nprint(f\"  ‚Ä¢ feature_correlations.png - Visualizaci√≥n de correlaciones\")\n\nprint(f\"\\nüéØ CARACTER√çSTICAS DEL DATASET:\")\nprint(f\"  ‚Ä¢ Target: is_urgent_a (Definici√≥n A: Percentil 75)\")\nprint(f\"  ‚Ä¢ Balance: {df['is_urgent_a'].mean():.1%} urgencias\")\nprint(f\"  ‚Ä¢ Horizonte temporal: {df['week_start'].nunique()} semanas\")\nprint(f\"  ‚Ä¢ Split: {len(df_train)} train / {len(df_test)} test\")\n\nprint(f\"\\n‚úÖ DATASET LISTO PARA MODELADO\")\nprint(f\"\\nPr√≥ximos pasos:\")\nprint(f\"  ‚Üí Fase 4: Modelado con Prophet\")\nprint(f\"  ‚Üí Fase 5: Modelado con XGBoost\")\nprint(f\"  ‚Üí Fase 6: Modelado con Random Forest\")\nprint(f\"  ‚Üí Fase 7: Comparaci√≥n de modelos\")\nprint(\"=\"*70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}